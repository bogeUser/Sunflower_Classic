# 机器学习
## 1. KNN和K-Means聚类有何不同
**KNN**
KNN算法(K-nearest Neighbor)是第一个有监督的分类算法，它需要有带标签的学习数据，但是它没有明显的前期训练过程，不需要对数据进行训练。而是在程序开始时把数据加载到内存中，直接开始分类。它的原理是通过计算新数据与训练数据特征之间的距离，然后选取K(k>=1)个距离最近的邻居进行分类或者回归。若k=1，新数据被简单分配给其近邻类。

步骤: 
- 计算测试数据与各个训练数据之间的距离；可以使用欧式距离的公式来进行计算
- 按照距离递增关系进行排序
- 选取距离最小的k个点
- 确定经k个点所在类别的出现频率
- 返回前k个点中出现频率最高的类别作为测试数据的预测分类

优点: 简单、有效
缺点: 计算量比较大，输出的模型可释性不强，需要存储全部的训练样本

**K-Means**
K-Means算法是一种聚类算法，它是很典型的基于距离的聚类算法，采用距离作为相似性评价指标，即认为两个对象的距离越近，其相似度越大。它是使用欧式距离度量的，它可以处理大数据集，而且高效。聚类结果是划分为k个数据集。

原理:
原始的K-Means算法首先随机选取k个点作为初始聚类的中心，然后计算各个数据对象到各个聚类中心的距离，把数据对象归到离它最近的那个聚类中心所在的类；调整后的新类计算新类的聚类中心，如果相邻两次聚类中心没有任何变化，说明数据对象调整结束，聚类准则函数已经收敛。在每次迭代中都要考察每个样本分类是否正确，若不正确，就要调整。在全部数据调整完后，再修改聚类中心，进入下一次迭代。如果在一次迭代算法中，所有的数据对象被正确分类，则不会有调整，聚类中心也不会有任何变化，这标志着聚类函数已经收敛，算法结束。

**区别**
KNN是分类算法，属于监督算法，喂给它的数据是带标签的完全正确的数据。它没有明显的训练过程。
K-Means是聚类算法，属于非监督学习算法，喂给它的数据集是无标签的，杂乱无章的数据，经过聚类后才变得有点顺序，有明显的训练过程。
## 2. 什么是贝叶斯定理？它在机器学习中有何作用
贝叶斯是根据一种数据的内容变化更新概率的方法。贝叶斯的核心是将两种不同的分布(似然和先验)组合成一个”更智能的“分布(即后验)。贝叶斯是一种机器学习思想，而不是简单的套用公式。而且在用朴素贝叶斯方式进行机器学习时还经常要使用一些辅助手段。

**公式:**
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$
其中:
- P(A)表示A出现的概率
- P(B|A)表示事件A发生的情况下，事件B发生的概率
- P(A|B)表示事件B发生的情况下，事件A发生的概率

## 3. 概率和似然有何区别
概率描述了已知参数时，随机变量的输出结果；似然则用来描述已知随机变量输出结果时，未知参数的可能取值。例如，对于“一枚正反的硬币抛10次”这种事件，可以问硬币落地时十次都是正面向上的概率是多少；而对于“一枚硬币上抛十次”,则可以问，这枚硬币正反面对称的似然度是多少。

- 似然是给定概率条件下，对观察对象的预测，也就是概率的逆反。
- 概率表达给定θ下样本随机向量X=x的可能性，而似然表达给定样本X=x下参数θ(相对于另外的参数$$θ_2$$)为真实性的可能性。一般对随机变量的取值谈概率。在贝叶斯统计的角度下，参数是一个实数而非随机变量，所以一般不谈概率，而说是似然。

## 4. 什么是傅里叶变换
傅里叶变换是将一般函数分解成堆成函数叠加的一般方法。傅里叶变化找到一组循环速度、振幅和相位，以及匹配任何时间信号。傅里叶变换就将信号从时间阈转换成频率阈。这是从音频信号或其他时间序列中提取特征的一种常见方法。

## 5. 如何对决策树进行剪枝
- 决策树每个节点选择某个特征作为划分节点的判断依据是熵值。根节点的划分是通过信息增益进行划分的，第一步先根据标签计算系统熵值，第二步根据每个特征值计算在以该特征为根节点的情况下的信息增益，信息增益作为根节点。
- 剪枝是在决策树中，为了降低模型的复杂度，提高决策树的预测精度，去除预测能力较弱的分支的一种操作，可以是自下而上的自上而下的，方法包括减少错误修剪和成本复杂度修剪。
- 预剪枝: 预剪枝的思想是在数种节点进行生长之前，先计算当前划分是否能提升模型的泛化能力，如果不能的话，那么就不再进行子树生长。一般来说，预剪枝对于何时停止决策树的生长有几种办法。1. 当决策树达到一定的深度时，停止生长。2. 当达到当前节点的样本数量小于某个阈值时，停止树的生长。3. 计算决策树每次分裂丢测试集的准确度是否提升，当没有提升或者提升度小于某个阈值时，停止决策树生长。预剪枝具有思想直接、算法简单、效率高等特点，适合解决大规模的问题，但是如何准确地轨迹何时停止决策树的生长，针对不同问题应该分别考虑，需要一定的判断经验。
- 后剪枝: 后剪枝的思想其实就是让算法生成一颗完全生长的决策树，然后从底层向上计算是否剪枝，如果需要剪枝，剪枝过程就是把子树删除，用一个叶子节点替代，该节点的类别按照多数投票的方法进行判断。同样的，后剪枝也可以通过在测试集上准确进行判断，如果剪枝后的准确率有所提升或者没有降低，那么就可以进行剪枝。一般来说，后剪枝通常得到的决策树泛化能力更好，但是时间上开销更大。
## 6. 如何评估机器学习模型的有效性
- 首先需要将数据集分成测试集和验证集，或者使用交叉验证的方法进行分割。然后需要选择度量模型表现的metrics，如F1-score、准确率、混淆矩阵等。更重要的是，根据实际情况去度量模型的轻微差别，一般与选择正确的度量标准。
- 模型的评估方法主要分为离线评估和在线评估。并且这对分类、回归、排序预测等不同类型的机器学习问题，评估指标也应该不同。
- 评估应分为:
    - 性能度量
        - 分类问题
        准确率、召回率、精确率、F1-score、RoC、AuC曲线。
        - 回归回归
        均方误差、均方根误差、均方根对数误差、平均绝对误差
    - 平行评估方法
    - 泛化能力
    - 过拟合、欠拟合的程度
    - 超参数调优
        
## 7. Boosting和Bagging的异同
- 二者都是集成学习算法，都是将多个学习器组合成强学习器的方法
- Bagging: 从原始数据集中每一轮有放回的抽取训练集，训练得到k个学习器，将这k个学习器以投票的方式得到最终的分类结果
- Boosting: 每轮从上一轮的分类结果动态调整每个样本在分类器中的权重，训练得到k个弱分类器，他们都有各自的权重，通过加权组合的方式得到最终的分类结果。

## 8. 对于二分类问题，定义超阈值t的判断为正例，否者判断为负例。如果将t增大，则准确率和召回率将如何变化
准确率 = TP / (TP + FP)， 召回率 = TP / (FP + FN)，其中TP表示将正例正确分类为正例的数量，FP表示将负例错误分类为正例的数量，FN表示将正例错误分类为负例的数量。准确率可以理解为在所有分类为正例的样品中，分类为正确样本所占比例；召回率可以理解为在所有原始数据中的正例样品中，正确挑出的正例样本的比例。
因此，如果增大阈值t，更多不确定的样本将被分类为负例，剩余确定(分类概率较大)的样本所占比例将会增大(或不变)，即正确率会增大(或不变)；若正大阈值t，则可能将部分不确定(分类概率较小)的正例样品分类为负，即召回率会减小(或不变)。

## 9. 解释SVM核函数的原理
核函数将数据映射到更高维的空间后处理，但不同做这种显示映射，而是先对两个样本向量做内积，然后用核函数映射。这等价于先进行映射，然后再做内积。

## 10. 哪些机器学习算法不需要做归一化处理
在实际应用中，需要做归一化的模型:
- 基于距离计算的的模型: KNN
- 通过梯度下降算法求解的模型: 线性回归、逻辑回归、支持向量机、神经网络
- 树形模型不需要做归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、随机森林

## 11. 树形结构为什么不需要做归一化
- 因为数值缩放不影响分裂点位置，对树形模型的结构不造成影响
- 按照特征值记性排序的，排序顺序不变，那么所属的分支以及分裂点就不会有不同。而且树模型是不能进行梯度下降的，因为构建树模型(回归树)寻找最优点是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没有意义，也就不需要归一化

## 12. 在K-Means或KNN中，常用欧式距离来计算邻居之间的距离，有时也用曼哈顿距离，对比这两种距离的差别
- **欧式距离**: 计算的是空间中两个点之间的真实距离，欧式距离越小，两个向量的相似度越大；反之，欧式距离越大，两个向量之间的相似度越小。
- **曼哈顿距离**: 计算的是两个点之间的直线距离。
- **区别**: 欧式距离和曼哈顿距离的区别在于他们对向量之间差异的计算过程，各个维度差异权值不同。向量各个属性之间的差距越大，则曼哈顿距离越接近欧式距离。

## 13. LR与线性回归的区别
- LR工业上一班值logistic Regression(逻辑回归)。LR在线性回归的实数范围上输出上施加Sigmoid函数将值收敛到0-1范围，其目标函数也因此从平方和函数变为对数损失函数，以提供最优化所需导数(Sigmoid函数是softmax函数的二元特例，其导数均为数值)
- LR往往是解决二元0/1分类问题的，只是他和线性回归耦合太紧，不自觉也冠了个回归的名字。如果要分类，就要把Sigmoid换成softmax函数了。

## 14. 请问(决策树、Random Forest、Boosting、Adaboot) GBDT和XGBoost的区别是什么
- 集成学习的集成对象是学习器。Bagging和Boosting属于集成学习的两类方法。Bagging方法有放回的采样相同数量样本训练训练每个学习器，然后再一起集成(简单投票)；Boosting方法使用全部样本(可调权重)依次训练每个学习器，迭代集成(平滑加权)
- 决策树属于最常用的学习器，其学习过程时从根建立树，也就是如何决策叶子结点分裂。ID3/C4.5决策树用信息熵计算最优分解，CART决策树用基尼指数计算足有分裂，Xgboost决策树使用二阶泰勒展开系数计算最优分解

## 15. xgboost要用泰勒展开，优势在哪
xgboost使用了一阶和二阶偏导，二阶偏导数有利于梯度幅下降更快更准。使用泰勒展开取得函数做自变量的二阶导数形式，可以在不选定损失函数具体形式的情况下，紧紧依靠输入数据的值就可以进行叶子分裂优化计算，本质上也就把损失函数的选取和模型算法优化/参数选择分开了。这种耦合增加了xgboost的适用性，使得它按需选取损失函数，可以用于分类，也可以用于回归。

## 16. 什么是最小二乘法
最小二乘法(又称为最小平方法)是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得位置数据，并使得这些求得的数据与实际数据之间误差的平方和最小。

## 17. 梯度下降法找到的一定是下降最快的方向么
梯度下降法并不一定是全局下降最快的方向，它只是目标函数在当前的点的切面上下降最快的方向。

## 18. 朴素贝叶斯的优势和劣势
- **优势**: 
    - 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率
    - 对大数量训练和查询时具有较高的速度。即使使用差大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已
    - 对小规模的数据表现很好，能处理多个分类任务，适合增量式训练(即可以实时的对新增的样本进行训练)
    - 对确实数据不太敏感，算法也比较简单，常用于文本分类
    - 朴素贝叶斯对结果解释容易理解
- **劣势**
    - 又有使用了样本你属性独立的假设，所以如果样本属性有关联时其效果不好(解释: 数据样本之间独立同分布意味着各个数据样本点之间没有依赖关系，也没有时序关系)，是从同一个分布经过多次采样得到的。如果不是同分布，是由多个分布产生的，那就是混合模型了，典型的如高斯混合模型；如果不独立，样本之间存在某种关系，那就需要把这种依赖搞关系建模进模型，如几天的天气情况可以使用隐马可夫模型进行网络建模

## 19. 请详细谈谈朴素贝叶斯

## 20. 什么是深度学习，机器学习和深度学习的区别在哪

## 21. 什么是TF-IDF
TF-IDF即词频-逆文档频率，是一种用于检索文本挖掘的常用加权技术。计算方式是TF*IDF。其中TF是词频，是一个词在文章中出现的次数，一个词如果在文章中出现的次数越多，说明这个词在文章中的作用就相对越大(停用词除外)。IDF是逆文档频率，由总文件数除以包含该词语的文件数据。IF-IDF的主要思想是，如果某个单词在一篇文章中出现的词频TF越高，并在其他文章中贯穿的越少，则认为该词频或者短语具有很好的类别区分能力，比较适合用来分类。

## 22. 机器学习中为什么需要梯度下降

## 23. 线性判别分析(LDA)的思想
LDA 是一种监督学习的降维技术，它的数据集的每个样本是有类别的输出的。它的目标是降维后达到内部数据尽可能的接近，即使得内部数据的方差最小，不同类别的数据之间的距离尽可能大，即类间方差最大。

## 24. LDA和PCA的区别
- **相同点**: 
    - 两者均是对数据进行降维 
    - 两者在降维时均使用了矩阵特征分解的思想
    - 两者都假设数据符合高斯分布
- **不同点**:
    - LDA是有监督的降维技术，而PCA是无监督的降维方法
    - LDA降维最多降维到类别书k-1的维数，而PCA没有限制
    - LDA除了可以用于降维，还可以用于分类
    - LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向

## 25. 主成分分析的思想
- 主成分分析是一种非监督的数据降维算法，利用正交换把一系列可能线性相关的变量转换为一种线性不相关的新变量，从而利用新变量在更小的维度上展示数据的特性。通过降维之后的额数据可以便于人理解，加快对样本有价值信息的处理速度
- PCA分析书先把演示变量做一个评估，计算各个变量各自的方差和两两变量之间的协方差，得到一个协方差矩阵。在这个协方差矩阵中，对角线的值为每个变量的方差，其它值为每两个变量的协方差。随后对原变量的协方差矩阵对角化处理，即求解其特征值和特征向量。原变量与特征向量的乘积即为新变量；新变量的协方差矩阵为对角线协方差矩阵且对角线的方差由大到小排列，然后从新变量中选择方差最大(方差最大意味着信息最丰富)的前2个或前3个新变量。

## 26. PCA算法的优缺点
- **优点**: 
    - 使得数据更容易使用
    - 降低算法的计算开销
    - 使得及诶过更容易理解
    - 完全无参数限制
- **缺点**: 
    - 如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得到预期效果，效率也不高。
    - 特征值分解有一些局限性，比如变换的矩阵必须是方阵
    - 在非高斯分布的情况下，PCA方法得出额主元可能并不是最优解

## 27. LDA算法的优缺点

## 28. PCA流程总结
1. 对数据进行归一化处理
2. 计算归一化后数据集的协方差矩阵
3. 计算协方差矩阵的特征值和特征向量
4. 保留组重要的K个特征
5. 找出K个特征值相应的向量
6. 将m*n的数据集乘以k个n维的特征向量，得到最后降维的数据

## 28. KPCA和PCA的区别

## 29. 常见的集成算法
**集成算法就是把多个算法模型组合在一起，力求达到更好的效果**
- **Bagging**: 并行训练多个分类器模型，然后取平均值，其中最典型的代表就是随机森林，随机森林就是将多个决策树模型放在一起。
- **Boosting**: 从弱学习器开始加强，通过加权来进行训练，Boosting是一个串行的集成算法，首先训练出一个模型，然后计算误差，在误差的基础上训练另一个模型，以此类推，直到达到要求。其中比较典型的模型就是AdaBoost和Xgboost。

## 30 简要介绍一下SVM
SVM是一个面向数据的分类算法，他的目标是为确定一个分类的超平面，从而将不同的数据分开。支持向量机学下方法包括构建由简至繁的模型：线性可分支持向量机、线性支持向量机以及非线性支持向量机。当训练数据线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化，也学习一个线性分类器，即线性支持向量机，又称为软间隔支持向量机；当训练书籍线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

## 31.GBDT和XGBoost的区别是什么
**XGBoost类似于GBDT的优化版，不论是精度还是效率上都有了提升。于GBDT相比，具体优点有:**
- 损失函数是用泰勒展示二项逼近，而不是像GBDT里的就是一阶导数
- 对树的结构哦进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性
- 节点分裂的方式不同，GBDT使用的基尼系数，XGBoost是经过优化后推导的

## 32. 在K-Means或KNN，使用欧式距离来计算最近的邻居之间的距离，为什么不用曼哈顿距离
曼哈顿距离只计算水平或垂直距离，有维度限制。另一方面，欧式距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧式距离是更可行的选择。

## 33. LR和SVM的区别和联系
- **联系**
    - LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题(在改进的情况下可以处理多分类问题)
    - 两个方法都可以增加不同的正则化，如L1、L2等等。所以在很多实验中，这两种算法的结果是很接近的

- **区别**
    - LR是参数模型，SVM是非参数模型
    - 从目标函数看，区别在于逻辑回归采用的事Logistical Loss，SVM采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，相对提升了与分类最相关的数据点的权重
    - 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算
    - Logic能做的SVM都能做，但可能在准确率问题上SVM能做的Logic有的做不了

## 34. LR于线性回归的区别和联系
逻辑回归和线性回归首先都是广义的线性回归。其次，经典线性模型的优化目标函数是最小二乘法，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围需要在[0, 1]。逻辑回归就是一种较小预测范围，将预测值限定为[0, 1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归要好。逻辑回归模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到Sigmoid的非线性形式，Sigmoid可以轻松处理0/1分类问题。

# 35. 为什么XGBoost要用泰勒展开，优势在哪
XGBoost使用了一阶导和二阶偏导，二阶导数有利于梯度下降的更快更准。使用泰勒展开取得二阶导数形式，可以在不选定损失函数具体形式的情况下用于优化算法分析。本质上也就把损失函数的选取和模型算法/参数选择分开了。这种去耦合增加了XGBoost的适用性。

## 36. XGBoost如何寻找最优特征；是有放回的还是无放回的
XGBoost在训练的过程中能给出各个特征的评分，从而表名每个特征对模型训练的重要性。XGBoost利用梯度优化模型算法，样本是不放回的。但是XGBoost支持子采样，也就是每轮计算可以不使用全部样本

## 37. 谈谈判别式模型和生成式模型
**判别方法**：
- 有数据直接学习决策函数Y = f(x)，或者由条件概率P(y|x)作为预测模型，即判别模型。常见的判别模型有: K近邻、SVM、决策树、感知机、线性判别分析(LDA)、线性回归、传统的神经网络、逻辑回归、boosting、条件随机场。
**生成方法**
- 由数据联合概率密度分布函数P(x,y)，然后给出条件概率分布P(y|x)作为预测的模型，即生成模型。常见的生成模型有: 朴素贝叶斯、隐马可夫模型、高斯混合模型、文档主题生成模型(LDA)、限制玻尔兹曼机。

***生成模型可以得到判别模型，但是判别模型得不到生成模型***

## 38. 线性分类器和非线性分类器的区别以及优劣
**如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否者就不是。**
- 常见的线性分类器有: LR、贝叶斯分类、单层感知机、线性回归。
- 常见的非线性分类器有: 决策树、RF、GBDT、多层感知机、SVM。

## 39. GAN
**生成对抗网络的核心思想是**: 同时训练两个相互协作，同时又相互竞争的深度神经网络(一个成为生成器Generator，另一个称为判别器Discriminator)来处理无监督学习的相关问题。

**判别器**: 判别器就是判断一个模型生成图像和真实图像比有多逼真。它的基本结构是卷积神经网络。网络的输出使用Sigmoid函数，输出值在[0, 1]，表示图像真实度的概率。其中0表示肯定是假的，1表示肯定是真的。

**生成器**: 生成器的作用是用来合成假的图像，使用卷积的导数，即转置卷积，生成假的图像。





