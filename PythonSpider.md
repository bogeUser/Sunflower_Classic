# PythonSpider

## 1. 如何理解网络爬虫

爬虫就是在网上定向抓取特定网站网页的HTML数据，常用的方法就是定义一个入口页面，然后一般一个页面会有其他页面的URL，然后从当前页面获取这些URL加入到爬虫爬取的队列中，然后进入新的页面重复进行上述操作。最后将爬取下来的数据做数据清洗，然后再根据实际需求做相关的后续操作。

## 2. 用爬虫爬取数据之后如何进行数据清洗

## 3. 简单介绍一下scrapy框架

- scrapy是个快速、高层次的基于python开发的web爬虫框架，用于抓取web站点并从网页中提取结构化的数据。scrapy使用了Twisted异步网络库来处理网络通讯。
- scrapy是用python编写的快速、高层次的网络爬虫框架，它使用了Twisted异步网络框架做通讯处理。它由7个大的部分组成。分别是scrapy Engine、Scheduler、Downloader、Spider、Item Pipeline、Downloader Middleware、Spider Middleware。

## 4. 为什么要使用scrapy框架？scrapy框架有哪些优点

- **它更容易构建大规模的抓取项目**
- **它是异步处理请求，速度快**
- **可以使用自动调节机制自动调整爬行速度**

## 5. scrapy框架有哪几个组件/模块? 简单说一下工作流程

- **Scrapy Engine**：是引擎，负责Spider、ItemPipline、Downloader、Scheduler间的通讯、信号、数据传递等等。
- **Scheduler**：调度器，他负责接收引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队、并等待Scrapy Engine来请求时，交由Scrapy Engine处理
- **Downloader**：下载器，负责下载Scrapy Engine发送过来的所有Request请求，并将其获取到的Response交还给Scrapy Engine，由Scrapy Engine转交给Spider来做进一步处理
- **Spider**：蜘蛛，负责处理所有Response，从中分析提取出数据，获取Item字段需要的数据，并将需要跟进的URL提交给Scrapy Engine，再由Scrapy Engine转交给Scheduler
- **Item Pipeline**：项目管道，负责处理Spider中获取到的Item，并进行处理，比如去重、持久化存储等
- **Downloader Middleware**：下载器中间件，用于自定义扩展的功能组件
- **Spider Middleware**： 爬虫中间件，一个可以自定义扩展和操作引擎和Spider中间“通信”的功能组件(比如进入Spider的Response；和从Spider出去的Request)

## 6. scrapy 如何实现分布式抓取

可以借助scrapy_redis类库来实现。在分布式爬取时，会有master机器和slave机器，其中，master为核心服务器，slave为具体的爬虫服务器。在master服务器上搭建一个redis数据库，并将要抓取的url存放到redis数据库中，所有的slave爬虫服务器在抓取的时候从redis数据库中取链接，由于scrapy_redis自身的队列机制，slave获取的url不会相互冲突，然后抓取的结果最后都存储到数据库中。master的redis数据库中还会将抓取过的url指纹存储起来，用于去重。去重是把Request的fingerprint存在redis上来实现的。

## 7. 爬虫使用多线程好？还是多进程好？为什么

对于IO密集型代码(文件处理，网络爬虫)，多线程能提升效率(单线程下有IO操作会进行IO等待，会造成不必要的时间等待，而开启多线程后，A线程等待时，会自动切换到线程B，可不浪费CPU资源，从而提高效率)在实际采集过程中，既考虑网速和相应的问题，也要考虑自身机器硬件的情况，来设计多进程和多线程。

## 8. 使用正则表达式、lxml、BeautifulSoup、selenium、jsonpath, xpath提取网页数据有何区别

- **正则表达式**: 使用正则表达式提取数据的速度很快，但是如果网页比较复杂的话使用正则表达式的难度比较大lxml只会做局部遍历。
- **BeautifulSoup**: BeautifulSoup，意味美味的汤，一碗好烫的煲制时间和用料上可定要更加费时，在编程上也一样，它的语义比Xpath强，但是他的速度没有Xpath快，因为Xpath的底层是用C语言来实现的，但总体来说还是比较友好的，功能也很强大但是，如果在很追求效率和资源节约的情况下，选用Xpath会比较好。

## 9. 使用redis搭建分布式系统时如何处理网络延迟和网络异常

由于网络异常的存在，分布式系统中请求结果存在”三态“的概念, 也就是三种状态，分别是 ”成功“， ”失败“， ”超时(或未知)“当出现 ”超时“ 时可以通过发起读取数据的操作以验证RPC是否成功。另一种简单的做法是，设计分布式协议时将执行步骤设计为可重试，即具有所谓的”幂等性。

## 10. 爬取下来的数据如何去重，说一下具体的算法依据

- 通过MD5生成电子指纹来判断也能是否改变
- nutch去重。nutch中的digest是对采集的每个网页内容的32位哈希值，如果两个网页内容完全一样，他们的digest值肯定一样

## 11. numpy和pandas的区别？分别的应用场景

- **numpy**numpy是数值计算的扩展包，主要用在纯数学计算方面
- **pandas**: pandas是做数据处理以矩阵为基础的数学计算模块。提供了一套名为DataFrame的数据结构，比较契合统计分析中的表结构，并且提供了计算接口，可以用numpy或其它方式进行计算

## 12. 验证码如何处理

- 可以通过scrapy框架进行处理，因为scarpy框架自带处理验证码功能
-  获取到验证码图片的url，调用第三方付费接口破解验证码

## 13. scrpay去重

- 1、当数据量不大是，可以直接放在内存中去重，python可以使用set() 进行去重
- 2、当去重数据需要持久化时可以使用redis 的set数据结构
- 3、当数据量再大一点时，可以用不同的加密算法先将长字符串压缩成16/32/40个字符，在使用上面两种方法去重
- 4、当数据量达到亿级时，内存有限，可以使用”位“来去重，才能满足需求。Bloomfilter就是将去重对象映射到几个内存 ”位“，通过几个位的0/1值来判断一个对象是否存在。但是Bloomfilter运行在一台机器的内存上，不方便持久化(如果down机的话就什么都没了)，也不方便分布式同一去重。如果可以在Redis上申请内存进行Bloomfilter，就可以解决这个问题了
-  5、simhash将一个文档最后转换成一个64位字节，暂且称之为征字节，然后判断重复只需要判断他们的征字节的距离是不是<n，就可以判断两个文档是否相似

## 14. celery、beanstalk，gearman分布式中那种分布式方案比较好

**个人认为gearman比较好。因为:**

- 技术类型简单，维护成本低
- 简单至上。能满足当前的技术需求即可(它可以满足分布式任务处理，异步同步任务同时支持，任务对列的持久化)
- 有成熟的使用案例。instagram就是使用gearman来完成图片的处理的相关任务，有成功的经验，当然可以借鉴

## 15. 谈谈你对selenium和phantomJS的了解

- **selenium**: selenium 是一个web自动测试框架，可以自动加载页面，获取需要的数据，甚至页面截图，或者判断网站上某些动作是否发生。selenium自己不带浏览器，不支持浏览器的功能，它需要与第三方
  浏览器结合在一起才能使用。但是我们有时候需要让他内嵌在代码中运行，所以可以用PhantomJS的工具代替真实的浏览器。selenium库里有个WebDriver的API。可以加载浏览器，它也可以像beautiful soup 或者其他selector对象一样用来查找页面元素，与页面上的元素进行交互(比如发送文本、点击)等，以及执行其他动作来运行网络爬虫
- **PhantomJS**: PhantomJS是一个基于Webkit的”无界面“浏览器，它会把网站加载到内存并执行页面上的JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效
- 如果把selenium和PhantomJS结合在一起，就可以运行一个非常强大的网络爬虫，这个爬虫几乎可以处理JavaScript、Cookie、headers等浏览器要执行的其它任务

## 16. 常见的反爬虫应对方法

- **通过Headers反爬虫**: 从用户请求的Heathers反爬虫是比较常见的发爬虫策略。很多网站都会对Headers的User-Agent进行检测，还有一部分网站会对referer进行检测(一些资源网站的防盗链就是检测Referer)。如果遇到了这类反爬虫机制，就可以直接在爬虫中添加Headers，将浏览器的User-Agent复制到爬虫的Headers中；或者将Referer值修改为目标网站域名。对于检测Headers的反爬虫，在爬虫中修改或者添加Headers就能很好的绕过。
- **基于用户行为的反爬虫**: 还有一部分网站是通过检测用户行为，例如同一IP短时间内多次访问同一页面，或者同一账号短时间内多次进行相同操作。大多数网站都是通过IP进行检测。对于这种情况通过使用IP代理就可以解决。可以专门写一个就爬虫，爬取网上公开的代理IP，检测后全部保存。在做爬虫时，可以通过设定一定的时间后更换ip，这在requests库中或者scrapy框架中都可以做到。对于账号问题，可以设定请求时间，即随机间隔一定的时间后再进行爬取。有些有逻辑错误的网站可以通过请求几次，退出登录，然后重新登录进行爬取。
- **动态页面的反爬虫**对于Headers、IP、账号，这类反爬虫通常是用在静态页面中，有的动态网站是通过Ajax请求得到数据，或者通过JavaScript生成的。首先用Fiddler对网络请求进行分析。如果能够找到Ajax请求，也能分析出具体的参数和响应的具体含义。然后通过参数重新构造url再发送请求。如果Ajax请求的所有参数进行了加密，可以根据selenium+PhantomJS，调用浏览器内核，并利用PhantomJS执行JS里模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求响应过程，只是完整的把人浏览页面获取数据的过程模拟一遍。

## 17. 分布式爬虫主要解决什么问题

- ip
- 带宽
- cpu
- IO

## 18. scrapy的优缺点

- **优点**：scrapy是异步的 采用可读性更强的xpath代替正则强大的统计log系统，同时在不同的url上进行爬行。支持shell方式，方便独立调试写middleware，方便一些统一的顾虑，通过管道的方式存入数据库。
- **缺点**：基于python的爬虫框架，扩展性比较差，基于Twisted框架，运行中的exception是不会干掉reactor，并且异步框架出错后是不会停掉其它任务的，数据出错难以察觉。

## 19. scrapy 框架的运行机制

- 从start_urls里获取第一批url并发送请求，请求由引擎交给调度器入请求对列，获取完毕后，调度器将请求对列里的请求交给下载器去获取请求对应的响应资源，并将响应交给自己编写的解析方法
  做提取处理：
  - 如果提取出需要的数据，则交给管道文件处理
  - 如果提取出url，则继续执行之前的步骤(发送url请求，并由引擎将请求交给调度器如队列)，直到请求队列里没有请求，程序结束

## 20. scrapy和scrapy-redis有什么区别

scrapy是一个python爬虫框架，爬取效率极高，具有高度定制性，但是不支持分布式。而scrapy-redis是一套基于redis数据库、运行在scrapy框架之上的组件，可以让scrapy支持分布式策略，Slaver端共享Master端redis数据库里的item队列、请求队列和请求指纹集合。选择redis数据库主要是因为redis支持主从同步，而且数据都是缓存在内存中的，所以基于redis的分布式爬虫，对请求和数据的高频读取效率非常高。

## 21. 实现模拟登录的方式有哪些

- 使用一个具有登录状态的cookie，结合请求报头一起发送，可以直接发送get请求，访问登录后才能访问的页面。
- 先发送登录界面的get请求，在登录页面HTML里获取登录需要的数据(如果需要的话)，然后结合账户密码，再发送post请求，即可登录成功。然后根据获取的cookie信息，接续访问之后的页面。

## 22. 简单介绍一下scrapy的异步处理

scrapy框架的异步处理机制是基于Twisted异步网络框架处理的，在settings.py文件里可以设置具体的并发数量(默认并发数量是16)。

## 23. 在scrapy框架使用选择器的时候使用css方法和xpath方法有何区别

## 24. 分布式爬虫的优点

- 能充分利用多机器的带宽加速爬取
- 充分利用多主机的IP加速爬取速度

## 25. 为什么scrapy不支持分布式

由于scrapy是单机模式，即scrapy Engine是通过一个scheduler，将Request对列中的Request请求发送给下载器，进行页面爬取，而scheduler是运行在队列中的，而队列是在单机内存中，服务器上的爬虫是无法利用内存的队列做任何处理。所以scrapy不支持分布式。

## 26. 分布式爬虫如何进行去重

去重主要是避免多台主机访问同一个Request请求，那么就需要用到Redis提供的队列结构。利用redis提供集合数据结构，在redis集合中存储每个Request指纹，在向Request队列中加入Request时首先验证指纹是否存在。如果存在，则说明已经有了该Request请求。

## 27. 如何防止分布式中断

可以通过启动判断，在每台slave的Scrapy启动而时候都会判断当前redis Request队列是否为空，如果不为空，则从队列中获取下一个Request进行爬取。如果为空则重新开始爬取。

## 28. 分布式爬虫对列中什么维护

一般是通过Redis队列进行维护，Redis是非关系型数据库，以Key-Value的形式进行存储，结构灵活。是内存中的数据解耦股存储系统，处理速度快，性能好。提供队列、集合等多种存储结构，方便对列进行维护。

## 29. 介绍一下动态页面的反爬有哪些措施，如何应对

## 30. 什么是增量式爬虫

## 31. 爬虫处理cookie和session的好处和坏处

- **好处**: 带上cookie、session之后能够请求到登录之后的页面
- **坏处**: 一套cookie和session往往和一个用户对应，请求太快、请求次数太多，容易被服务器识别为爬虫

## 32. linux系统中，如何确定分布式爬虫已经死亡

